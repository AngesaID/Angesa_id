{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Check Indonesia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Pre-Trained Fasttext Model Indonesia\n",
    "Pre-trained Fasttext model yang diunduh adalah hasil pelatihan menggunakan metode CBOW dengan *position-weight*, dalam dimensi 300, dengan panjang karakter n-gram sebesar 5, *window size* 5 dan *negatives* 10. Link kumpulan pre-trained model Fasttext dapat diunduh [disini](https://fasttext.cc/docs/en/crawl-vectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "def bar_progress(current, total, width=80):\n",
    "    progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
    "     # Don't use print() as it will print in new line every time.\n",
    "    sys.stdout.write(\"\\r\" + progress_message)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.bin.gz'\n",
    "\n",
    "# filename = wget.download(url, bar=bar_progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract model dengan menggunakan `gunzip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# with gzip.open('cc.id.300.bin.gz', 'rb') as f_in:\n",
    "#     with open('cc.id.300.bin', 'wb') as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import fasttext\n",
    "from gensim.models.fasttext import load_facebook_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fasttext Model using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# model = fasttext.load_facebook_model('cc.id.300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pada pre-trained model Fasttext terdapat 2.000.000 (dua juta) *vocabulary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('models.pkl', 'rb') as file:\n",
    "    vocab = pickle.load(file)\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '.', '</s>', 'yang', 'dan', '\"', 'di', ')', '(', 'dengan']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('models.pkl', 'wb') as file:\n",
    "#     pickle.dump(vocab, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index for each word in Vocabulary\n",
    "Buat *dictionary word-rank* pada tiap kata/karakter yang terdapat pada *vocabulary*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 715 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "words = list(vocab)\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "    \n",
    "WORDS = w_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 0,\n",
       " '.': 1,\n",
       " '</s>': 2,\n",
       " 'yang': 3,\n",
       " 'dan': 4,\n",
       " '\"': 5,\n",
       " 'di': 6,\n",
       " ')': 7,\n",
       " '(': 8,\n",
       " 'dengan': 9}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "dict(itertools.islice(WORDS.items(), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peter Norvig Spelling Corrector\n",
    "Salah satu metode paling sederhana untuk Spelling Corrector adalah dengan metode yang diterapkan oleh [Peter Norvig.](https://norvig.com/spell-correct.html)\n",
    "\n",
    "### Perbedaan\n",
    "Terdapat perbedaaan pada code Peter Norvig dengan code CPMP untuk penerapan Spell Check. Perbedaannya jika menggunakan **code Peter Norvig dengan menghitung frekuensi dari tiap kata dari kamus**, sedangkan **code CPMP dengan menggunakan ranking dari Word2Vec model**.\n",
    "\n",
    "### Cara Kerja\n",
    "Spell Check ini menggunakan Teorema Bayes untuk menemukan *correction c* dengan memilih *probability* terbesar dari semua *probability candidate correction*. Metode tersebut dapat dijabarkan menjadi 4 bagian:\n",
    "1. **Selection Mechanism**: `argmax`, dengan memilih *candidate* yang memiliki *probability* terbesar.\n",
    "2. **Candidate Model:** `c âˆˆ candidates`, didapatkan *candidate corrections c*, untuk dipertimbangkan.\n",
    "3. **Languange Model**: `P(c)`, *probability* dari kemunculan *candidate corretion c* pada kamus. Pada penerapannya menggunakan kamus Word2Vec model.\n",
    "4. **Error Model**: `P(w|c)`, *probability* apabila yang diketik adalah kata *w* sedangkan yang dimaksud adalah *c*. Sebagai contoh, *probability* `P(mkan|makan)` lebih tinggi dibandingkan dengan *probability* `P(mkanxxxyz|makan)` yang lebih rendah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kucing'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction(\"kcing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jakarta'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction('J4karta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'menyedihkan'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction('mnyedihknn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slang + Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"source/slang.txt\") as f:\n",
    "    slangS = json.loads(f.read())\n",
    "    \n",
    "type(slangS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def join_punctuation(seq, characters='.,;?!'):\n",
    "    characters = set(characters)\n",
    "    seq = iter(seq)\n",
    "    current = next(seq)\n",
    "\n",
    "    for nxt in seq:\n",
    "        if nxt in characters:\n",
    "            current += nxt\n",
    "        else:\n",
    "            yield current\n",
    "            current = nxt\n",
    "\n",
    "    yield current\n",
    "\n",
    "def slang(T):\n",
    "    Texts = re.findall(r\"[\\w']+|[.,!?;]\",T)\n",
    "    \n",
    "    _spelling = []\n",
    "    for text in Texts:\n",
    "        _spelling.append(correction(text))\n",
    "    \n",
    "    for index,text in enumerate(_spelling):\n",
    "        if text in slangS.keys():\n",
    "            _spelling[index] = slangS[text]\n",
    "    \n",
    "    print(_spelling)\n",
    "    _text = list(join_punctuation(_spelling))\n",
    "    print(_text)\n",
    "    _text = ' '.join(join_punctuation(_spelling))\n",
    "    return _text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FATHURHUARWHU'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['FATHURHUARWHU']\n",
    "\n",
    "''.join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'kamu', 'sayang', '.', 'serius', 'juragan', '!', ',', 'tapi', 'tidak', 'tahu', 'kalau', 'besok', '.']\n",
      "['I', 'love', 'kamu', 'sayang.', 'serius', 'juragan!,', 'tapi', 'tidak', 'tahu', 'kalau', 'besok.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I love kamu sayang. serius juragan!, tapi tidak tahu kalau besok.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang(\"I luv u say. serius gan!, tapi ndak tau kalau sesok.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sinonim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('source/dict.json') as f:\n",
    "    mydict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSinonim(word):\n",
    "    if word in mydict.keys():\n",
    "        return mydict[word]['sinonim']\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def getAntonim(word):\n",
    "    if word in mydict.keys():\n",
    "        if 'antonim' in mydict[word].keys():\n",
    "            return mydict[word]['antonim']\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aman', 'bahagia', 'bangga', 'berbungabunga ', 'berkenan', 'bungah', 'camar', 'ceria', 'doyan', 'enak', 'gemar', 'gembira', 'girang', 'lapang dada', 'lega', 'makmur', 'meriah', 'nikmat', 'nyaman', 'puas', 'ria', 'riang', 'sejahtera', 'semarak', 'selesa', 'suka', 'sukacita', 'sukaria', 'tenang', 'tenteram']\n"
     ]
    }
   ],
   "source": [
    "print(getSinonim('senang'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['duka', 'getir ', 'gundah', 'lara', 'masygul', 'menyesak', 'merana', 'pedih', 'pilu', 'prihatin', 'sedu', 'susah hati', 'terharu', 'trenyuh']\n"
     ]
    }
   ],
   "source": [
    "print(getSinonim(getAntonim('senang')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "- https://github.com/louisowen6/NLP_bahasa_resources#pos-tagging\n",
    "- https://medium.com/@yasirabd/spell-check-indonesia-menggunakan-pre-trained-fasttext-model-14e90a3f1ac0\n",
    "- https://norvig.com/spell-correct.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4465b56ab07b4b37dd2120fd6e12a5c69113d1fe3c5a02503d475be473c9d9e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('myenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
